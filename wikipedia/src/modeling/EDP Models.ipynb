{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Distribution Predictors\n",
    "\n",
    "We may be losing a lot of information in the annotations by condensing them into a single number. Instead, we can train a model to predict the empirical distribution formed by the annotations over the answer choices. We do this by minimizing the cross-entropy between the predicted distributions and the empirical distributions. This is essentially softmax classification, but off-the-shelf implementations don't let you pass a distribution as a training label, so we have to roll out own in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from ngram import *\n",
    "from baselines import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_filename  = '../../data/v4_annotated/annotated_onion_layer_5_rows_0_to_5000_raters_20.csv'\n",
    "d = load_cf_labels(data_filename)\n",
    "d = tidy_labels(d)\n",
    "d = d.dropna(subset=['attack'])\n",
    "d = d.iloc[np.random.permutation(np.arange(d.shape[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_feature_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range = (1,6), max_features = 5000)),\n",
    "    ('tfidf', TfidfTransformer(sublinear_tf=True,norm='l2')),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Plurality\n",
    "\n",
    "Fit a softmax regression to the most common annotation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.826558864\n",
      "Accuracy: 0.55903\n",
      "Accuracy: 0.541542\n",
      "Epoch: 0016 cost= 0.645986644\n",
      "Accuracy: 0.666333\n",
      "Accuracy: 0.647648\n",
      "Epoch: 0031 cost= 0.538006697\n",
      "Accuracy: 0.730865\n",
      "Accuracy: 0.693694\n",
      "Epoch: 0046 cost= 0.461819182\n",
      "Accuracy: 0.776138\n",
      "Accuracy: 0.733734\n",
      "Epoch: 0061 cost= 0.405223148\n",
      "Accuracy: 0.823162\n",
      "Accuracy: 0.747748\n",
      "Epoch: 0076 cost= 0.361157354\n",
      "Accuracy: 0.851926\n",
      "Accuracy: 0.762763\n",
      "Epoch: 0091 cost= 0.325631864\n",
      "Accuracy: 0.875688\n",
      "Accuracy: 0.76977\n",
      "Epoch: 0106 cost= 0.296221656\n",
      "Accuracy: 0.892196\n",
      "Accuracy: 0.775776\n",
      "Epoch: 0121 cost= 0.271312231\n",
      "Accuracy: 0.907704\n",
      "Accuracy: 0.77978\n",
      "Epoch: 0136 cost= 0.249868573\n",
      "Accuracy: 0.917459\n",
      "Accuracy: 0.777778\n",
      "Epoch: 0151 cost= 0.231153783\n",
      "Accuracy: 0.929715\n",
      "Accuracy: 0.785786\n",
      "Epoch: 0166 cost= 0.214671709\n",
      "Accuracy: 0.938469\n",
      "Accuracy: 0.788789\n",
      "Epoch: 0181 cost= 0.199999733\n",
      "Accuracy: 0.945473\n",
      "Accuracy: 0.790791\n",
      "Epoch: 0196 cost= 0.186850882\n",
      "Accuracy: 0.953227\n",
      "Accuracy: 0.791792\n",
      "Epoch: 0211 cost= 0.174963344\n",
      "Accuracy: 0.957229\n",
      "Accuracy: 0.791792\n",
      "Epoch: 0226 cost= 0.164198468\n",
      "Accuracy: 0.961231\n",
      "Accuracy: 0.795796\n",
      "Epoch: 0241 cost= 0.154424270\n",
      "Accuracy: 0.966233\n",
      "Accuracy: 0.798799\n",
      "Epoch: 0256 cost= 0.145410911\n",
      "Accuracy: 0.969985\n",
      "Accuracy: 0.802803\n",
      "Epoch: 0271 cost= 0.137158165\n",
      "Accuracy: 0.974237\n",
      "Accuracy: 0.803804\n",
      "Epoch: 0286 cost= 0.129572296\n",
      "Accuracy: 0.977989\n",
      "Accuracy: 0.804805\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "labels = plurality(d['attack'])\n",
    "\n",
    "data = get_labeled_comments(d, labels)\n",
    "train, test = split(data, test_size = 0.2,)\n",
    "\n",
    "y_train =train.ix[:, train.columns != 'x'].values[:,0]\n",
    "y_test =test.ix[:, train.columns != 'x'].values[:,0]\n",
    "y_train = np.array([y_train, 1- y_train]).T\n",
    "y_test = np.array([y_test, 1- y_test]).T\n",
    "\n",
    "ngram_feature_extractor = ngram_feature_pipeline.fit(train['x'])\n",
    "X_train = ngram_feature_extractor.transform(train['x'])\n",
    "X_test = ngram_feature_extractor.transform(test['x'])\n",
    "\n",
    "ED_CLF(X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        training_epochs = 300,\n",
    "        batch_size = 200,\n",
    "        display_step = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Empirical Distribution\n",
    "Fit a softmax regression to the empirical distribtion of annotions over answer choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.860558868\n",
      "Accuracy: 0.646573\n",
      "Accuracy: 0.668669\n",
      "Epoch: 0016 cost= 0.715179900\n",
      "Accuracy: 0.674087\n",
      "Accuracy: 0.671672\n",
      "Epoch: 0031 cost= 0.643569165\n",
      "Accuracy: 0.731616\n",
      "Accuracy: 0.698699\n",
      "Epoch: 0046 cost= 0.596841331\n",
      "Accuracy: 0.775388\n",
      "Accuracy: 0.71972\n",
      "Epoch: 0061 cost= 0.564251946\n",
      "Accuracy: 0.809155\n",
      "Accuracy: 0.733734\n",
      "Epoch: 0076 cost= 0.540559257\n",
      "Accuracy: 0.833667\n",
      "Accuracy: 0.742743\n",
      "Epoch: 0091 cost= 0.522752478\n",
      "Accuracy: 0.855678\n",
      "Accuracy: 0.760761\n",
      "Epoch: 0106 cost= 0.509014375\n",
      "Accuracy: 0.872436\n",
      "Accuracy: 0.772773\n",
      "Epoch: 0121 cost= 0.498218831\n",
      "Accuracy: 0.884942\n",
      "Accuracy: 0.771772\n",
      "Epoch: 0136 cost= 0.489562381\n",
      "Accuracy: 0.896198\n",
      "Accuracy: 0.777778\n",
      "Epoch: 0151 cost= 0.482511312\n",
      "Accuracy: 0.905203\n",
      "Accuracy: 0.77978\n",
      "Epoch: 0166 cost= 0.476710386\n",
      "Accuracy: 0.913957\n",
      "Accuracy: 0.780781\n",
      "Epoch: 0181 cost= 0.471877582\n",
      "Accuracy: 0.922961\n",
      "Accuracy: 0.786787\n",
      "Epoch: 0196 cost= 0.467798205\n",
      "Accuracy: 0.928964\n",
      "Accuracy: 0.785786\n",
      "Epoch: 0211 cost= 0.464327448\n",
      "Accuracy: 0.933967\n",
      "Accuracy: 0.784785\n",
      "Epoch: 0226 cost= 0.461340740\n",
      "Accuracy: 0.938219\n",
      "Accuracy: 0.786787\n",
      "Epoch: 0241 cost= 0.458765965\n",
      "Accuracy: 0.94072\n",
      "Accuracy: 0.78979\n",
      "Epoch: 0256 cost= 0.456512432\n",
      "Accuracy: 0.944472\n",
      "Accuracy: 0.790791\n",
      "Epoch: 0271 cost= 0.454533192\n",
      "Accuracy: 0.947224\n",
      "Accuracy: 0.794795\n",
      "Epoch: 0286 cost= 0.452791979\n",
      "Accuracy: 0.948724\n",
      "Accuracy: 0.792793\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "labels = empirical_dist(d['attack'], w = 0.5)\n",
    "data = get_labeled_comments(d, labels)\n",
    "train, test = split(data, test_size = 0.2,)\n",
    "\n",
    "y_train =train.ix[:, train.columns != 'x'].values\n",
    "y_test =test.ix[:, train.columns != 'x'].values\n",
    "\n",
    "ngram_feature_extractor = ngram_feature_pipeline.fit(train['x'])\n",
    "X_train = ngram_feature_extractor.transform(train['x'])\n",
    "X_test = ngram_feature_extractor.transform(test['x'])\n",
    "\n",
    "ED_CLF(X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        training_epochs = 300,\n",
    "        batch_size = 200,\n",
    "        display_step = 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
