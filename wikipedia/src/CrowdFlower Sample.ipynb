{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from mw_api_diff_utils import *\n",
    "import pandas as pd\n",
    "from db_utils import query_hive_ssh\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk Page Diff Sampling\n",
    "\n",
    "In this notebook we will be taking samples from the full set of user talk page diffs for annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bot Filtering\n",
    "\n",
    "We don't want to include messages from bots in our sample.\n",
    "\n",
    "### Resources\n",
    "- [overview](https://www.mediawiki.org/wiki/Manual:Bots)\n",
    "- [users in bot group](https://en.wikipedia.org/w/index.php?title=Special:ListUsers/bot&offset=BrokenAnchorBot&group=bot)\n",
    "- [user group table docs](https://www.mediawiki.org/wiki/Manual:User_groups_table)\n",
    "- [recent chnages table docs](https://www.mediawiki.org/wiki/Manual:Recentchanges_table)\n",
    "\n",
    "### Summary\n",
    "There is no great way to determine bots. Here are 3 strategies:\n",
    "\n",
    "1. The `recentchnages` table contains 30 days of data and tells you which revisions where made by bots. This is the most reliable data, but there may be a seasonal trend in harassment that biases the sample.\n",
    "\n",
    "2. Check if the user is in the bot group in the `user group` table. Drawbacks are:\n",
    "    - Group membership can change over time. There are many bot-flagged edits by users that are no longer in a user group providing the bot right. Likewise there are many edits not bot-flagged by users that now have the bot right (which they may or may not use for each edit).\n",
    "    - Not all bots are a member of this group (there are other groups that provide this right, sysop, for example).\n",
    "3. Remove users with 'bot|Bot|BOT' in the user_name. Almost all bots in the 'bot' user group fit this pattern, but there will be many false positives. It seems unlikely that these false positives result in biased sample ...\n",
    "\n",
    "For now I will go with option 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "For now, we basically just strip markup and remove duplicate diffs.\n",
    "\n",
    "##### Steps\n",
    "1. remove dupilicate rev_ids\n",
    "3. remove duplicate diffs\n",
    "4. clean diffs\n",
    "     - ignore diffs with no content added\n",
    "     - replace 'NEWLINE' with '\\n'\n",
    "     - strip mw markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "months = ['January',\n",
    "          'February',\n",
    "          'March',\n",
    "          'April',\n",
    "          'June',\n",
    "          'July',\n",
    "          'August',\n",
    "          'September',\n",
    "          'October',\n",
    "          'November',\n",
    "          'December',\n",
    "        ]\n",
    "\n",
    "\n",
    "month_or = '|'.join(months)\n",
    "date_p = re.compile('\\d\\d:\\d\\d, \\d?\\d (%s) \\d\\d\\d\\d \\(UTC\\)' % month_or)\n",
    "    \n",
    "def remove_date(comment):\n",
    "    return re.sub(date_p , '', comment )\n",
    "\n",
    "\n",
    "\n",
    "pre_sub_patterns = [\n",
    "                    ('\\[\\[Image:.*?\\]\\]', ''),\n",
    "                    ('<!-- {{blocked}} -->', ''),\n",
    "                    ('NEWLINE', '\\n'),\n",
    "                    ('\\[\\[File:.*?\\]\\]', ''),\n",
    "                    ('\\[\\[User:.*?\\|.*?\\]\\]', ''),\n",
    "                    ('\\(\\[\\[User talk:.*?\\|talk\\]\\]\\)', ''),\n",
    "                   ]\n",
    "\n",
    "post_sub_patterns = [\n",
    "                    ('--', ''),\n",
    "                    (' :', ' '),\n",
    "                    ]\n",
    "\n",
    "def substitute_patterns(s, sub_patterns):\n",
    "    for p, r in sub_patterns:\n",
    "        s = re.sub(p, r, s)\n",
    "    return s\n",
    "\n",
    "def strip_html(s):\n",
    "    s = BeautifulSoup(s, 'html.parser').get_text()\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean(df):\n",
    "    df = copy.deepcopy(df)\n",
    "    df.rename(columns = {'insertion': 'diff'}, inplace = True)\n",
    "    df.dropna(subset = ['diff'], inplace = True)\n",
    "    df['clean_diff'] = df['diff']\n",
    "    \n",
    "    df['clean_diff'] = df['clean_diff'].apply(remove_date)\n",
    "    df['clean_diff'] = df['clean_diff'].apply(lambda x: substitute_patterns(x, pre_sub_patterns))\n",
    "    df['clean_diff'] = df['clean_diff'].apply(strip_mw)\n",
    "    df['clean_diff'] = df['clean_diff'].apply(strip_html)\n",
    "    df['clean_diff'] = df['clean_diff'].apply(lambda x: substitute_patterns(x, post_sub_patterns))\n",
    "\n",
    "\n",
    "    try:\n",
    "        del df['rank']\n",
    "    except:\n",
    "        pass\n",
    "    #df.drop_duplicates(subset = ['rev_id'], inplace = True)\n",
    "    #df.drop_duplicates(subset = ['diff'], inplace = True)\n",
    "    df.dropna(subset = ['clean_diff'], inplace = True)\n",
    "    df = df[df['clean_diff'] != '']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_comments(clean(df[:100]), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin Messages\n",
    "### Administrative Message Types Observed\n",
    "\n",
    "- responses to to valdalism \n",
    "- new user greetings\n",
    "- 5 million articles celebration\n",
    "- speedy deletion\n",
    "- deletion\n",
    "- unfree files\n",
    "- article for creation:\n",
    "- comments with embedded image tags that contain 'warning' or 'information'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_comments(d, n = 10):\n",
    "    for i, r in d[:n].iterrows():\n",
    "        print(r['diff'])\n",
    "        print('_' * 80)\n",
    "        print(r['clean_diff'])\n",
    "        print('\\n\\n', '#' * 80, '\\n\\n')\n",
    "\n",
    "def find_pattern(d, pattern, column):\n",
    "    p = re.compile(pattern)\n",
    "    return d[d[column].apply(lambda x: p.search(x) is not None)]\n",
    "\n",
    "def exclude_pattern(d, pattern, column):\n",
    "    p = re.compile(pattern)\n",
    "    return d[ d[column].apply(lambda x: p.search(x) is None)]\n",
    "\n",
    "def exclude_few_tokens(d, n):\n",
    "    return d[d['clean_diff'].apply(lambda x: len(x.split(' ')) > n)]\n",
    "\n",
    "def exclude_short_strings(d, n):\n",
    "    return d[d['clean_diff'].apply(lambda x: len(x) > n)]  \n",
    "\n",
    "def remove_admin(d, patterns):\n",
    "    d_reduced = copy.deepcopy(d)\n",
    "    for pattern in patterns:\n",
    "        d_reduced = exclude_pattern(d_reduced, pattern, 'diff')\n",
    "    return d_reduced\n",
    "\n",
    "patterns =[\n",
    "    '\\[\\[Image:Octagon-warning',\n",
    "    '\\[\\[Image:Stop',\n",
    "    '\\[\\[Image:Information.',\n",
    "    '\\[\\[Image:Copyright-problem',\n",
    "    '\\[\\[Image:Ambox',\n",
    "    '\\[\\[Image:Broom',\n",
    "    '\\[\\[File:Information',\n",
    "    '\\[\\[File:AFC-Logo_Decline',\n",
    "    '\\[\\[File:Ambox',\n",
    "    '\\[\\[File:Nuvola',\n",
    "    '\\[\\[File:Stop',\n",
    "    '\\[\\[File:Copyright-problem',\n",
    "    '\\[\\[File:Copyright-problem',\n",
    "    '\\[\\[File:Copyright-problem',\n",
    "    '\\[\\[File:Copyright-problem',\n",
    "    '\\|alt=Warning icon\\]\\]',\n",
    "    'The article .* has been \\[\\[Wikipedia:Proposed deletion\\|proposed for deletion\\]\\]',\n",
    "    'Your submission at \\[\\[Wikipedia:Articles for creation\\|Articles for creation\\]\\]',\n",
    "    'A file that you uploaded or altered, .*, has been listed at \\[\\[Wikipedia:Possibly unfree files\\]\\]',\n",
    "    'User:SuggestBot',\n",
    "    '\\[\\[Wikipedia:Criteria for speedy deletion\\|Speedy deletion\\]\\] nomination of',\n",
    "    \"Please stop your \\[\\[Wikipedia:Disruptive editing\\|disruptive editing\\]\\]. If you continue to \\[\\[Wikipedia:Vandalism\\|vandalize\\]\\] Wikipedia, as you did to .*, you may be \\[\\[Wikipedia:Blocking policy\\|blocked from editing\\]\\]\",\n",
    "    \"Hello.*and.*\\[\\[Project:Introduction\\|welcome\\]\\].* to Wikipedia!\",\n",
    "    'Nomination of .* for deletion',\n",
    "    '== Welcome to Wikipedia! ==',\n",
    "    '== Welcome! ==',\n",
    "    '== 5 Million: We celebrate your contribution ==',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#d['diff'].apply(lambda x: re.search('\\[\\[Image(.*?)\\]\\].*', x)).dropna().apply(lambda x:x.group(1)).value_counts().head(50)\n",
    "#d['diff'].apply(lambda x: re.search('\\[\\[File:(.*?)\\]\\].*', x)).dropna().apply(lambda x:x.group(1)).value_counts().head(50)\n",
    "#show_comments(find_pattern(d,'\\|alt=Warning icon\\]\\]', 'diff' ), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Based Random Sample\n",
    "Consider comments made since min_timestamp. Take n random comments from non-bot users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 100000\n",
    "min_timestamp = '2014-03-01T00:00:00Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    diffs.*\n",
    "FROM\n",
    "    enwiki.user_talk_diff diffs\n",
    "WHERE\n",
    "    rev_timestamp > '%(min_timestamp)s'\n",
    "    AND user_text != 'MediaWiki message delivery'\n",
    "    AND user_text != 'Maintenance script'\n",
    "    AND user_text NOT RLIKE 'bot|Bot|BOT'\n",
    "ORDER BY RAND()\n",
    "LIMIT %(n)d\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    'n': n,\n",
    "    'min_timestamp': min_timestamp\n",
    "    }\n",
    "\n",
    "df = query_hive_ssh(query % params, 'post_sample.tsv', priority = True)\n",
    "df.columns = [c.split('.')[1] for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: 100000\n",
      "Cleaned:  91169\n",
      "No Admin 48650\n",
      "No Few Words:  40169\n",
      "No Few Chars:  38849\n"
     ]
    }
   ],
   "source": [
    "print('Raw:', df.shape[0])\n",
    "clean_df = clean(df)\n",
    "print('Cleaned: ', clean_df.shape[0])\n",
    "reduced_df = remove_admin(clean_df, patterns)\n",
    "print('No Admin', reduced_df.shape[0])\n",
    "reduced_df = exclude_few_tokens(reduced_df, 3)\n",
    "print('No Few Words: ', reduced_df.shape[0])\n",
    "reduced_df = exclude_short_strings(reduced_df, 20)\n",
    "print('No Few Chars: ', reduced_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_df[:1000].to_csv('../data/1k_post_sample.tsv', sep = '\\t')\n",
    "clean_df.to_csv('../data/all_post_sample.tsv', sep = '\\t')\n",
    "reduced_df[:1000].to_csv('../data/1k_no_admin_post_sample.tsv', sep = '\\t')\n",
    "reduced_df.to_csv('../data/all_no_admin_post_sample.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_comments(d_reduced, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Based Random Sample\n",
    "\n",
    "Consider comments made since min_timestamp. Take up to k random comments from each of n random non-bot users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "n = 10000\n",
    "min_timestamp = '2014-03-01T00:00:00Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    revisions.*\n",
    "FROM\n",
    "    (SELECT\n",
    "        a.*\n",
    "        FROM\n",
    "        (\n",
    "        SELECT\n",
    "            d.*,\n",
    "            RANK() OVER (PARTITION BY d.user_id ORDER BY RAND()) as rank\n",
    "        FROM\n",
    "            enwiki.user_talk_diff d\n",
    "        WHERE \n",
    "            rev_timestamp > '%(min_timestamp)s'\n",
    "        ) a\n",
    "    WHERE rank <= %(k)d\n",
    "    )\n",
    "    revisions\n",
    "\n",
    "JOIN\n",
    "    (\n",
    "    SELECT\n",
    "        a.*\n",
    "    FROM (\n",
    "        SELECT\n",
    "            user_id,\n",
    "            RAND() as key\n",
    "        FROM\n",
    "            enwiki.user_talk_diff d\n",
    "        WHERE \n",
    "            rev_timestamp > '%(min_timestamp)s'\n",
    "            AND user_text != 'MediaWiki message delivery'\n",
    "            AND user_text != 'Maintenance script'\n",
    "            AND user_text NOT RLIKE 'bot|Bot|BOT'\n",
    "        GROUP BY user_id\n",
    "        ) a\n",
    "    ORDER BY key\n",
    "    LIMIT %(n)d\n",
    "    ) users\n",
    "ON\n",
    "    revisions.user_id = users.user_id\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    'k': k,\n",
    "    'n': n,\n",
    "    'min_timestamp': min_timestamp\n",
    "    }\n",
    "\n",
    "\n",
    "user_sample_df = query_hive_ssh(query % params, 'random_sample.tsv', priority = True)\n",
    "user_sample_df.columns = [c.split('.')[1] for c in user_sample_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEPRECATED ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocked Users\n",
    "For n random user block events that occured after min_timestamp, take up to the k most recent comments before being blocked for harassment or personal attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 25\n",
    "n = 10000\n",
    "min_timestamp = '2010-03-01T00:00:00Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "SELECT\n",
    "    revisions.*\n",
    "FROM\n",
    "    (\n",
    "    SELECT \n",
    "        diffs.*,\n",
    "        RANK() OVER (PARTITION BY events.key, events.user_text ORDER BY diffs.rev_timestamp DESC) as rank\n",
    "    FROM\n",
    "        (\n",
    "        SELECT\n",
    "            a.*\n",
    "        FROM\n",
    "            (\n",
    "            SELECT \n",
    "              log_title as user_text,\n",
    "              log_timestamp,\n",
    "              RAND() as key\n",
    "            FROM enwiki.logging \n",
    "            WHERE\n",
    "              log_type = 'block'\n",
    "              AND log_action = 'block'\n",
    "              AND log_comment RLIKE 'harassment|personal attack'\n",
    "              AND log_timestamp > '%(min_timestamp)s'\n",
    "            ) a\n",
    "        ORDER BY key\n",
    "        LIMIT %(n)d\n",
    "        ) events\n",
    "    JOIN\n",
    "        enwiki.user_talk_diff diffs\n",
    "    ON\n",
    "        diffs.user_text = events.user_text\n",
    "    WHERE\n",
    "        diffs.rev_timestamp <= events.log_timestamp\n",
    "    ) revisions\n",
    "WHERE\n",
    "    rank <= %(k)d\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    'k': k,\n",
    "    'n': n,\n",
    "    'min_timestamp': min_timestamp\n",
    "    }\n",
    "\n",
    "\n",
    "df = query_hive_ssh(query % params, 'blocked_user_random_sample.tsv', priority = True, delete = False)\n",
    "df.columns = [c.split('.')[1] for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: 6034\n",
      "Cleaned:  4857\n",
      "No Admin 4567\n",
      "No Few Words:  3719\n",
      "No Few Chars:  3634\n"
     ]
    }
   ],
   "source": [
    "print('Raw:', df.shape[0])\n",
    "clean_df = clean(df)\n",
    "print('Cleaned: ', clean_df.shape[0])\n",
    "reduced_df = remove_admin(clean_df, patterns)\n",
    "print('No Admin', reduced_df.shape[0])\n",
    "reduced_df = exclude_few_tokens(reduced_df, 3)\n",
    "print('No Few Words: ', reduced_df.shape[0])\n",
    "reduced_df = exclude_short_strings(reduced_df, 20)\n",
    "print('No Few Chars: ', reduced_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df[:1000].to_csv('../data/1k_blocked_user_post_sample.tsv', sep = '\\t')\n",
    "clean_df.to_csv('../data/all_blocked_user_post_sample.tsv', sep = '\\t')\n",
    "reduced_df[:1000].to_csv('../data/1k_no_admin_blocked_user_post_sample.tsv', sep = '\\t')\n",
    "reduced_df.to_csv('../data/all_no_admin_blocked_user_post_sample.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocked User Longitudinal Data\n",
    "For some users who got blocked for harassment/personal attacks, get all their user talk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(DISTINCT(user_text)) as n\n",
    "FROM\n",
    "    enwiki.blocked_user\n",
    "\"\"\"\n",
    "\n",
    "d = query_hive_ssh(query, 'qa.tsv', priority = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Users Blocked for Harassment\n",
      "4140\n"
     ]
    }
   ],
   "source": [
    "print('Num Users Blocked for Harassment')\n",
    "print(d['n'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Blocked Users with a revision in:  revision\n",
      "3739 4620989\n",
      "Num Blocked Users with a revision in:  user_talk_revision\n",
      "2886 577247\n",
      "Num Blocked Users with a revision in:  user_talk_diff\n",
      "2819 571473\n",
      "Num Blocked Users with a revision in:  blocked_user_talk_diff\n",
      "2819 571473\n"
     ]
    }
   ],
   "source": [
    "tables = [\n",
    "            ('revision', 'rev_user_text'),\n",
    "            ('user_talk_revision', 'rev_user_text'),\n",
    "            ('user_talk_diff', 'user_text'),\n",
    "            ('blocked_user_talk_diff', 'user_text'),\n",
    "    ]\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    b.user_text,\n",
    "    COUNT(*) as n\n",
    "FROM\n",
    "    enwiki.%s t\n",
    "JOIN\n",
    "    enwiki.blocked_user b\n",
    "ON\n",
    "    b.user_text = t.%s\n",
    "GROUP\n",
    "    BY b.user_text\n",
    "\"\"\"\n",
    "\n",
    "for t in tables:\n",
    "    print('Num Blocked Users with a revision in: ', t[0])\n",
    "    d = query_hive_ssh(query % t, 'qa.tsv', priority = True)\n",
    "    print(d.shape[0], d['n'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4.1k users who were blocked for harassment, but only 3.7k have any revision and only 2.8k have a user talk revision. Whats going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    b.user_text\n",
    "FROM\n",
    "    (\n",
    "    SELECT\n",
    "        DISTINCT(%(username)s) as user_text\n",
    "    FROM\n",
    "        enwiki.%(table)s \n",
    "    ) t\n",
    "RIGHT JOIN\n",
    "    enwiki.blocked_user b\n",
    "ON\n",
    "    t.user_text = b.user_text \n",
    "WHERE \n",
    "    t.user_text is NULL\n",
    "\"\"\"\n",
    "params = {\n",
    "    'username': 'rev_user_text',\n",
    "    'table': 'revision'\n",
    "}\n",
    "d = query_hive_ssh(query % params, 'qa.tsv', priority = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b.user_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.34.12.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.27.226.0/24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>117.197.156.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>142.162.218.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148.233.165.0/24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        b.user_text\n",
       "0      100.34.12.38\n",
       "1   103.27.226.0/24\n",
       "2   117.197.156.176\n",
       "3   142.162.218.188\n",
       "4  148.233.165.0/24"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I manually checked a bunch of examples in the prod DB and they really don't have any revisions. In some cases, the event happened off wiki (e.g. AOL, MSN). In other cases, maybe the revisions were deleted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling\n",
    "How many users can we sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    user_text,\n",
    "    COUNT(*) num_user_talk_revs\n",
    "FROM\n",
    "    enwiki.blocked_user_talk_diff\n",
    "GROUP\n",
    "    BY user_text\n",
    "\"\"\"\n",
    "\n",
    "user_talk_revs_per_user_df = query_hive_ssh(query, 'random_sample.tsv', priority = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     569\n",
       "2     316\n",
       "3     230\n",
       "4     166\n",
       "5     131\n",
       "6     108\n",
       "7      84\n",
       "8      77\n",
       "9      50\n",
       "10     47\n",
       "11     41\n",
       "12     37\n",
       "13     36\n",
       "15     32\n",
       "17     26\n",
       "16     25\n",
       "14     25\n",
       "21     21\n",
       "18     19\n",
       "27     18\n",
       "Name: num_user_talk_revs, dtype: int64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_talk_revs_per_user_df['num_user_talk_revs'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Users:  2819\n",
      "Num Comments:  571473\n",
      "Num Users with < 1000 Edits:  2726\n",
      "Num Comments from Users with  < 1000 Edits:  117024\n"
     ]
    }
   ],
   "source": [
    "k =1000\n",
    "print('Num Users: ', user_talk_revs_per_user_df.shape[0])\n",
    "print('Num Comments: ', user_talk_revs_per_user_df['num_user_talk_revs'].sum())\n",
    "\n",
    "print ('Num Users with < %d Edits: ' % k,  user_talk_revs_per_user_df[user_talk_revs_per_user_df['num_user_talk_revs'] < k].shape[0])\n",
    "print('Num Comments from Users with  < %d Edits: ' % k, user_talk_revs_per_user_df[user_talk_revs_per_user_df['num_user_talk_revs'] < k]['num_user_talk_revs'].sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CParserError",
     "evalue": "Error tokenizing data. C error: Expected 12 fields in line 146, saw 13\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCParserError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-df3200577f02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_hive_ssh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blocked_user_long.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriority\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ellerywulczyn/wmf/util/db_utils.py\u001b[0m in \u001b[0;36mquery_hive_ssh\u001b[0;34m(query, file_name, priority, delete, delim)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"ssh stat1002.eqiad.wmnet \"hive -e \\\\\" \"\"\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"\"\" \\\\\" \"> \"\"\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ellerywulczyn/miniconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[1;32m    496\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ellerywulczyn/miniconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m _parser_defaults = {\n",
      "\u001b[0;32m/Users/ellerywulczyn/miniconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skip_footer not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ellerywulczyn/miniconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.read (pandas/parser.c:7988)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_low_memory (pandas/parser.c:8244)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._read_rows (pandas/parser.c:8970)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._tokenize_rows (pandas/parser.c:8838)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.raise_parser_error (pandas/parser.c:22649)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCParserError\u001b[0m: Error tokenizing data. C error: Expected 12 fields in line 146, saw 13\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "SELECT\n",
    "    diffs.rev_comment,\n",
    "    SUBSTRING(diffs.insertion, 0, 1000) as insertion,\n",
    "    diffs.insert_only,\n",
    "    diffs.rev_id,\n",
    "    diffs.page_id,\n",
    "    diffs.page_title,\n",
    "    diffs.rev_timestamp,\n",
    "    diffs.user_id,\n",
    "    diffs.user_text,\n",
    "    diffs.block_reasons,\n",
    "    diffs.block_timestamps,\n",
    "    diffs.block_actions\n",
    "FROM\n",
    "    enwiki.blocked_user_talk_diff diffs\n",
    "JOIN\n",
    "    (\n",
    "    SELECT\n",
    "        user_text\n",
    "    FROM\n",
    "        enwiki.blocked_user_talk_diff\n",
    "    GROUP\n",
    "        BY user_text\n",
    "    HAVING \n",
    "        COUNT(*) < %(max_comments)d\n",
    "    ) users\n",
    "    \n",
    "ON\n",
    "    diffs.user_text = users.user_text\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    'max_comments': 1000\n",
    "    }\n",
    "\n",
    "\n",
    "df = query_hive_ssh(query % params, 'blocked_user_long.tsv', priority = True, delete = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n"
     ]
    }
   ],
   "source": [
    "# In case padas csv parser breaks\n",
    "f =  open('blocked_user_long.tsv')\n",
    "df = pd.DataFrame(columns = next(f).split('\\t'))\n",
    "for i, line in enumerate(f):\n",
    "    if i % 10000 ==0:\n",
    "        print(i)\n",
    "    df.loc[i] = line.split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns = [c.replace('diffs.', '') for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: 117024\n",
      "Cleaned:  104406\n",
      "No Admin 101979\n",
      "No Few Words:  84276\n",
      "No Few Chars:  82273\n"
     ]
    }
   ],
   "source": [
    "print('Raw:', df.shape[0])\n",
    "clean_df = clean(df)\n",
    "print('Cleaned: ', clean_df.shape[0])\n",
    "reduced_df = remove_admin(clean_df, patterns)\n",
    "print('No Admin', reduced_df.shape[0])\n",
    "reduced_df = exclude_few_tokens(reduced_df, 3)\n",
    "print('No Few Words: ', reduced_df.shape[0])\n",
    "reduced_df = exclude_short_strings(reduced_df, 20)\n",
    "print('No Few Chars: ', reduced_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_comments(clean_blocked_user_long_df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_df.to_csv('../data/all_blocked_user_long_sample.tsv', sep = '\\t')\n",
    "reduced_df.to_csv('../data/all_no_admin_blocked_user_long_sample.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = set(reduced_clean_blocked_user_long_df.index)\n",
    "s2 = set(clean_blocked_user_long_df.index)\n",
    "s3 = s2 - s1\n",
    "d_excluded = clean_blocked_user_long_df.ix[s3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_comments( find_pattern(d_excluded, patterns[0], 'diff'), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
