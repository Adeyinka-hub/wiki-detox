{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 7.01 ms\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "\n",
    "from baselines import get_annotator_ensemble_baselines_parallel,get_annotator_ensemble_baseline, get_model_baselines_parallel, get_model_baseline\n",
    "from baselines import multi_class_roc_auc, multi_class_spearman\n",
    "from baselines import empirical_dist\n",
    "from ngram import load_comments_and_labels,assemble_data\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from serialization import load_pipeline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.13 ms\n"
     ]
    }
   ],
   "source": [
    "iters = 25\n",
    "K = 20\n",
    "F = int(K/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "annotations = pd.read_csv('../../data/annotations/split/baseline/annotations.tsv', sep='\\t')\n",
    "annotations.index = annotations.rev_id\n",
    "comments = annotations.drop_duplicates('rev_id')['clean_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.41 ms\n"
     ]
    }
   ],
   "source": [
    "metrics = {'ROC': multi_class_roc_auc, 'spearman':multi_class_spearman}\n",
    "tasks = ['attack'] \n",
    "#tasks = ['attack' 'recipient', 'aggression']\n",
    "annotations_subsets = {'all': annotations} \n",
    "#annotations_subsets = {\n",
    "#    'all': d,\n",
    "#    'blocked' : annotations.query(\"sample=='blocked'\"),\n",
    "#    'random' : annotations.query(\"sample=='random'\")\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get annotator ensemble baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.45 ms\n"
     ]
    }
   ],
   "source": [
    "pairs = list(zip(range(1, F+1), range(1, F+1))) + list(zip(range(1, F+1), [F]*F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for task in tasks:\n",
    "    for annotations_subset_name, annotations_subset in annotations_subsets.items():\n",
    "        task_annotations  = annotations_subset[task]\n",
    "        for metric_name, metric in metrics.items():\n",
    "            args = [[task_annotations, K, empirical_dist, metric, n_t, n_p] for n_t, n_p in pairs] * iters\n",
    "            result = get_annotator_ensemble_baselines_parallel(args)\n",
    "            result['metric'] = metric_name\n",
    "            result['task'] = task\n",
    "            dfs.append(result)\n",
    "ensemble_baseline_results = pd.concat(dfs)\n",
    "ensemble_baseline_results['score'] = ensemble_baseline_results['score'] * 100\n",
    "ensemble_baseline_results = ensemble_baseline_results.groupby(['metric', 'n_p', 'n_t', 'task'])['score'].agg({\"mean\": np.mean, 'std': np.std, 'count': 'count'})\n",
    "pd.DataFrame(ensemble_baseline_results).to_csv('baselines1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th>n_p</th>\n",
       "      <th>n_t</th>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ROC</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>attack</th>\n",
       "      <td>0.649089</td>\n",
       "      <td>88.782649</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman</th>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <th>attack</th>\n",
       "      <td>1.660748</td>\n",
       "      <td>58.297410</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              std       mean  count\n",
       "metric   n_p n_t task                              \n",
       "ROC      2   2   attack  0.649089  88.782649      2\n",
       "spearman 2   2   attack  1.660748  58.297410      2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.17 ms\n"
     ]
    }
   ],
   "source": [
    "ensemble_baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "cv_path = '../../models/attack'\n",
    "model_name = 'linear_char_ed'\n",
    "\n",
    "for task in tasks:\n",
    "    model = load_pipeline(os.path.join(cv_path,model_name), model_name)\n",
    "    y_baseline_pred = model.predict_proba(comments)\n",
    "    y_baseline_pred = pd.DataFrame(y_baseline_pred,  index = comments.index)\n",
    "\n",
    "    for annotations_subset_name, annotations_subset in annotations_subsets.items():\n",
    "        task_annotations  = annotations_subset[task]\n",
    "        for metric_name, metric in metrics.items():\n",
    "            args = [[y_baseline_pred, task_annotations, K, empirical_dist, metric, F]] * iters\n",
    "            result = get_model_baselines_parallel(args)\n",
    "            result['metric'] = metric_name\n",
    "            result['task'] = task\n",
    "            dfs.append(result)\n",
    "model_baseline_results = pd.concat(dfs)\n",
    "model_baseline_results['score'] = model_baseline_results['score'] * 100\n",
    "model_baseline_results = model_baseline_results.groupby(['metric', 'n_t','task' ])['score'].agg({\"mean\": np.mean, 'std': np.std, 'count': 'count'})\n",
    "pd.DataFrame(model_baseline_results).to_csv('baselines2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th>n_t</th>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ROC</th>\n",
       "      <th>10</th>\n",
       "      <th>attack</th>\n",
       "      <td>0.018624</td>\n",
       "      <td>98.259263</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman</th>\n",
       "      <th>10</th>\n",
       "      <th>attack</th>\n",
       "      <td>0.162121</td>\n",
       "      <td>69.761587</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          std       mean  count\n",
       "metric   n_t task                              \n",
       "ROC      10  attack  0.018624  98.259263      2\n",
       "spearman 10  attack  0.162121  69.761587      2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.67 ms\n"
     ]
    }
   ],
   "source": [
    "model_baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
