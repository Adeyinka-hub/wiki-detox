{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import inspect, os\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "os.sys.path.insert(0,parentdir) \n",
    "from data_generation.diff_utils import clean_and_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_agg = joblib.load( '../../models/aggression_ngram.pkl')\n",
    "m_rec = joblib.load( '../../models/recipient_ngram.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load annotationed diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_models(df):\n",
    "    diffs = df['clean_diff']\n",
    "    agg_scores = m_agg.predict_proba(df['clean_diff'])\n",
    "    df['pred_aggression_score'] = agg_scores.dot(np.array([1, 0, -1]))\n",
    "    df['pred_aggressive'] = agg_scores[:, 0]\n",
    "    df['pred_neutral'] = agg_scores[:, 1]\n",
    "    df['pred_friendly'] = agg_scores[:, 2]\n",
    "    rec_scores = m_rec.predict_proba(df['clean_diff'])\n",
    "    df['pred_recipient_score'] = rec_scores[:,1]\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annotations = load_annotations()\n",
    "\n",
    "for ns in ['user', 'article']:\n",
    "\n",
    "    d_annotations = annotations[ns]['random']\n",
    "    \n",
    "    d_annotations['aggression'] = (d_annotations['aggression'] -1) * -1\n",
    "\n",
    "    d_annotated = d_annotations\\\n",
    "                .drop_duplicates(subset=['rev_id'])\\\n",
    "                .assign(\n",
    "                    recipient = plurality(d_annotations['recipient'].dropna()),\n",
    "                    recipient_score = average(d_annotations['recipient'].dropna()),\n",
    "                    aggression = plurality(d_annotations['aggression'].dropna()),\n",
    "                    aggression_score = average(d_annotations['aggression'].dropna()))\n",
    "\n",
    "    d_annotated.to_csv('../../data/samples/%s/clean/d_annotated.tsv' % ns, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load samples and apply models\n",
    "\n",
    "We take various diffs datasets from hive, apply the clean and filter function and the score the clean diffs using the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pred_helper(df):\n",
    "    if len(df) == 0:\n",
    "        return None\n",
    "    \n",
    "    return df.assign(rev_timestamp = lambda x: pd.to_datetime(x.rev_timestamp),\n",
    "                     clean_diff = lambda x: x['clean_diff'].astype(str))\\\n",
    "             .pipe(apply_models)\n",
    "\n",
    "    \n",
    "def prep_in_parallel(path, k = 8):\n",
    "    df = pd.read_csv(path, sep = '\\t', encoding = 'utf-8')\\\n",
    "           .assign(key = lambda x: np.random.randint(0, high=5*k, size=x.shape[0]))\n",
    "    dfs = [e[1] for e in df.groupby('key')]\n",
    "    p = mp.Pool(k)\n",
    "    dfs = p.map(pred_helper, dfs)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base = '../../data/samples/'\n",
    "nss = ['user', 'article']\n",
    "samples = ['talk_diff_no_admin_sample.tsv', 'talk_diff_no_admin_2015.tsv', 'all_blocked_user.tsv', 'd_annotated.tsv']\n",
    "\n",
    "base_cols = ['rev_id', 'clean_diff', 'rev_timestamp', 'pred_aggression_score', 'pred_recipient_score', 'page_title', 'user_text','user_id']\n",
    "extra_cols = ['recipient', 'recipient_score', 'aggression', 'aggression_score']\n",
    "\n",
    "for ns in nss:\n",
    "    for s in samples:\n",
    "        inf = os.path.join(base, ns, 'clean', s)\n",
    "        outf = os.path.join(base, ns, 'scored', s)\n",
    "        if s == 'd_annotated.tsv':\n",
    "            cols = base_cols + extra_cols\n",
    "        else:\n",
    "            cols = base_cols\n",
    "        prep_in_parallel(inf, k = 4)[cols].to_csv(outf, sep = '\\t', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
